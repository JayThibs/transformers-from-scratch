{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_lightning_transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMNUafGdTJEi8KjR2vxa7He",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/transformers-from-scratch/blob/main/pytorch_lightning_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers with PyTorch Lightning\n",
        "\n",
        "References:\n",
        "\n",
        "* [Simple PyTorch Transformer Example with Greedy Decoding](https://colab.research.google.com/drive/1swXWW5sOLW8zSZBaQBYcGQkQ_Bje_bmI) by Sergey Karayev from Full Stack Deep Learning\n",
        "* [The Annotated Transformer ++](https://github.com/gordicaleksa/pytorch-original-transformer/blob/main/The%20Annotated%20Transformer%20%2B%2B.ipynb) by gordicaleksa / The AI Epiphany\n",
        "* [Transformers from Scratch](https://e2eml.school/transformers.html) by End-to-End ML School\n",
        "* [Notes on GPT-2 and BERT models](https://www.kaggle.com/residentmario/notes-on-gpt-2-and-bert-models) by Aleksey Bilogur\n",
        "* [GPT-3: Language Models are Few-Shot Learners (Paper Explained)](https://www.youtube.com/watch?v=SY5PvZrJhLE) by Yannic Kilcher\n",
        "* [Various Annotated Transformer PyTorch Papers](https://nn.labml.ai/transformers/index.html) by labml.ai\n",
        "\n",
        "This notebook provides a simple, self-contained example of Transformer models:\n",
        "\n",
        "* using both the encoder and decoder parts\n",
        "* greedy decoding at inference time\n",
        "\n",
        "For the first part of the notebook, we'll train on a simple synthetic example, and use PyTorch Lightning since it will greatly simplify the training loop.\n",
        "\n",
        "When the first transformer paper came out (Attention Is All You Need), the authors used the transformer architecture for machine translation. This means that they needed both the encoder and decoder parts of the architecture to first encode the text, and then decode (generate) the translation.\n",
        "\n",
        "After that paper, researchers realized that they could use the encoder and decoder separately in order to create models for approaching different tasks. This led to the emergence of BERT-like models (encoder / non-autoregressive) and GPT-like models (decoder / autoregressive).\n",
        "\n",
        "However, we'll be going over each part of the entire transformer.\n",
        "\n",
        "Note: Autoregressive means that model only takes into account the text or context that came before our current prediction. Each new prediction is taken into account in the next prediction. Non-autoregressive models take the entire surrounding context! So, a model like BERT uses bi-directionality (that's what the B stands for) and takes in the entire surrounding context for word prediction when trying to predict a masked word. This makes it so that GPT is great at generating text, while BERT is great at taking in an entire piece of text and classifying it."
      ],
      "metadata": {
        "id": "S1mc18GM-tZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ],
      "metadata": {
        "id": "tmjycciQG_fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning spacy --quiet"
      ],
      "metadata": {
        "id": "3Cx0eeAlDfFw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "1rmwkVbtHPab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python native libs\n",
        "import math\n",
        "import copy\n",
        "import os\n",
        "import time\n",
        "import enum\n",
        "import argparse\n",
        "\n",
        "# Visualization imports\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "\n",
        "\n",
        "# Deep learning imports\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.hub import download_url_to_file\n",
        "\n",
        "# Data manipulation\n",
        "import numpy as np\n",
        "# from torchtext.data import Dataset, BucketIterator, Field, Example\n",
        "from torchtext.data.utils import interleave_keys\n",
        "from torchtext import datasets\n",
        "# from torchtext.data import Example\n",
        "import spacy\n",
        "\n",
        "# BLEU\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "metadata": {
        "id": "TLcKlC0iHNSl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "klbGQ8dvIrlA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}