{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_lightning_transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMVezkXmjxWO0qtyGxE7Yd7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/transformers-from-scratch/blob/main/pytorch_lightning_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers with PyTorch Lightning\n",
        "\n",
        "References:\n",
        "\n",
        "* [Simple PyTorch Transformer Example with Greedy Decoding](https://colab.research.google.com/drive/1swXWW5sOLW8zSZBaQBYcGQkQ_Bje_bmI) by Sergey Karayev from Full Stack Deep Learning\n",
        "* [The Annotated Transformer ++](https://github.com/gordicaleksa/pytorch-original-transformer/blob/main/The%20Annotated%20Transformer%20%2B%2B.ipynb) by gordicaleksa / The AI Epiphany\n",
        "* [Transformers from Scratch](https://e2eml.school/transformers.html) by End-to-End ML School\n",
        "* [Notes on GPT-2 and BERT models](https://www.kaggle.com/residentmario/notes-on-gpt-2-and-bert-models) by Aleksey Bilogur\n",
        "* [GPT-3: Language Models are Few-Shot Learners (Paper Explained)](https://www.youtube.com/watch?v=SY5PvZrJhLE) by Yannic Kilcher\n",
        "* [Various Annotated Transformer PyTorch Papers](https://nn.labml.ai/transformers/index.html) by labml.ai\n",
        "\n",
        "This notebook provides a simple, self-contained example of Transformer models:\n",
        "\n",
        "* using both the encoder and decoder parts\n",
        "* greedy decoding at inference time\n",
        "\n",
        "For the first part of the notebook, we'll train on a simple synthetic example, and use PyTorch Lightning since it will greatly simplify the training loop.\n",
        "\n",
        "When the first transformer paper came out (Attention Is All You Need), the authors used the transformer architecture for machine translation. This means that they needed both the encoder and decoder parts of the architecture to first encode the text, and then decode (generate) the translation.\n",
        "\n",
        "After that paper, researchers realized that they could use the encoder and decoder separately in order to create models for approaching different tasks. This led to the emergence of BERT-like models (encoder / non-autoregressive) and GPT-like models (decoder / autoregressive).\n",
        "\n",
        "However, we'll be going over each part of the entire transformer.\n",
        "\n",
        "Note: Autoregressive means that model only takes into account the text or context that came before our current prediction. Each new prediction is taken into account in the next prediction. Non-autoregressive models take the entire surrounding context! So, a model like BERT uses bi-directionality (that's what the B stands for) and takes in the entire surrounding context for word prediction when trying to predict a masked word. This makes it so that GPT is great at generating text, while BERT is great at taking in an entire piece of text and classifying it."
      ],
      "metadata": {
        "id": "S1mc18GM-tZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ],
      "metadata": {
        "id": "tmjycciQG_fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning spacy --quiet"
      ],
      "metadata": {
        "id": "3Cx0eeAlDfFw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4452dcea-ba01-4921-df45-005aa73abceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 525 kB 9.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 58.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 332 kB 70.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 61.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 54.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 56.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 65.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 160 kB 54.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 192 kB 70.9 MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "1rmwkVbtHPab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python native libs\n",
        "import math\n",
        "import copy\n",
        "import os\n",
        "import time\n",
        "import enum\n",
        "import argparse\n",
        "\n",
        "# Visualization imports\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "\n",
        "\n",
        "# Deep learning imports\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.hub import download_url_to_file\n",
        "\n",
        "# Data manipulation\n",
        "import numpy as np\n",
        "# from torchtext.data import Dataset, BucketIterator, Field, Example\n",
        "from torchtext.data.utils import interleave_keys\n",
        "from torchtext import datasets\n",
        "# from torchtext.data import Example\n",
        "import spacy\n",
        "\n",
        "# BLEU\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "metadata": {
        "id": "TLcKlC0iHNSl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "\n",
        "Since this notebook is focused on understanding the transformer architecture in code, we'll be generating simple input and output data for training a model.\n",
        "\n",
        "Input: An array of values where each element is repeated twice, e.g. [1, 1, 5, 5, 3, 3]\n",
        "\n",
        "Output: Same as input, but the duplicates are removed, e.g. [1, 5, 3]"
      ],
      "metadata": {
        "id": "YazRQDI6PmpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = 10000\n",
        "S = 32 # target/output sequence length. The input will be twice as long.\n",
        "C = 128 # number of \"classes\", including 0, the \"start token\", and 1, the \"end token\"\n",
        "\n",
        "Y = (torch.rand((N * 10, S - 2)) * (C - 2)).long() + 2 # Only generate ints in (2, 99) range\n",
        "\n",
        "# Make sure we only have unique rows\n",
        "Y = torch.tensor(np.unique(Y, axis=0)[:N])\n",
        "X = torch.repeat_interleave(Y, 2, dim=1)\n",
        "\n",
        "# Add special 0 \"start\" and 1 \"end\" tokens to beginning and end\n",
        "Y = torch.cat([torch.zeros((N, 1)), Y, torch.ones((N, 1))], dim=1).long()\n",
        "X = torch.cat([torch.zeros((N, 1)), X, torch.ones((N, 1))], dim=1).long()\n",
        "\n",
        "# Look at the data\n",
        "print(X, X.shape)\n",
        "print(Y, Y.shape)\n",
        "print(Y.min(), Y.max())"
      ],
      "metadata": {
        "id": "klbGQ8dvIrlA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c671a001-05b6-4cb5-8e08-c4d539ace228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  0,   2,   2,  ...,  16,  16,   1],\n",
            "        [  0,   2,   2,  ..., 102, 102,   1],\n",
            "        [  0,   2,   2,  ...,  49,  49,   1],\n",
            "        ...,\n",
            "        [  0,  14,  14,  ...,  29,  29,   1],\n",
            "        [  0,  14,  14,  ...,   8,   8,   1],\n",
            "        [  0,  14,  14,  ...,  46,  46,   1]]) torch.Size([10000, 62])\n",
            "tensor([[  0,   2,   2,  ...,  25,  16,   1],\n",
            "        [  0,   2,   2,  ...,  56, 102,   1],\n",
            "        [  0,   2,   2,  ..., 117,  49,   1],\n",
            "        ...,\n",
            "        [  0,  14,  70,  ...,  51,  29,   1],\n",
            "        [  0,  14,  70,  ..., 114,   8,   1],\n",
            "        [  0,  14,  70,  ..., 126,  46,   1]]) torch.Size([10000, 32])\n",
            "tensor(0) tensor(127)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap data in the simplest possible way to enable PyTorch data fetching\n",
        "# https://pytorch.org/docs/stable/data.html\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "TRAIN_FRAC = 0.8\n",
        "\n",
        "dataset = list(zip(X, Y)) # This fulfills the pytorch.utils.data.Dataset interface\n",
        "\n",
        "# Split into train and val\n",
        "num_train = int(N * TRAIN_FRAC)\n",
        "num_val = N - num_train\n",
        "data_train, data_val = torch.utils.data.random_split(dataset, (num_train, num_val))\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE)\n",
        "dataloader_val = torch.utils.data.DataLoader(data_val, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Sample batch\n",
        "x, y = next(iter(data_train))\n",
        "x, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svg4Fz8Qs3t2",
        "outputId": "d1a7061a-96a2-4a9e-dc97-b13fa0b53458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([  0,  10,  10,  96,  96, 122, 122,  81,  81,  64,  64,  75,  75,  26,\n",
              "          26,  61,  61,  23,  23,  65,  65,  40,  40, 100, 100,  24,  24, 120,\n",
              "         120,   7,   7, 110, 110, 107, 107, 118, 118, 126, 126,  28,  28,  91,\n",
              "          91, 119, 119,  67,  67,  29,  29,  45,  45,  74,  74,  62,  62, 114,\n",
              "         114,  28,  28,  28,  28,   1]),\n",
              " tensor([  0,  10,  96, 122,  81,  64,  75,  26,  61,  23,  65,  40, 100,  24,\n",
              "         120,   7, 110, 107, 118, 126,  28,  91, 119,  67,  29,  45,  74,  62,\n",
              "         114,  28,  28,   1]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "w9o8b2kRutCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Classic Attention-is-all-you-need positional encoding.\n",
        "    From PyTorch docs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arrange(0, max_len, dtype=torch.float).unsqueeze(1) # gives us the ordered position of words\n",
        "        div_term = torch.exp(torch.arrange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "def generate_square_subsequence_mask(size: int):\n",
        "    \"\"\"Generate a triangular (size, size) mask. From PyTorch docs.\"\"\"\n",
        "    mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float(-inf)).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Classic Transformer that both encodes and decodes.\n",
        "\n",
        "    Prediction-time inference is done greedily.\n",
        "\n",
        "    NOTE: Start token is hard-coded to be 0, end token to be 1. If changing, update predict() accordingly.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes: int, max_output_length:int, dim: int = 128):\n",
        "        super().__init__()\n",
        "\n",
        "        # Parameters\n",
        "        self.dim = dim\n",
        "        self.max_output_length = max_output_length\n",
        "        nhead = 4\n",
        "        num_layers = 4\n",
        "        dim_feedforward = dim\n",
        "\n",
        "        # Encoder part\n",
        "        self.embedding = nn.Embedding(num_classes, dim)\n",
        "        self.pos_encoder = PositionalEncoding(d_model=self.dim)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer = nn.TransformerEncoderLayer(d_model=self.dim, nhead=nhead, dim_feedforward=dim_feedforward),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        # Decoder part\n",
        "        self.y_mask = generate_square_subsequence_mask(self.max_output_length)\n",
        "        self.transformer_encoder = nn.TransformerDecoder(\n",
        "            decoder_layer = nn.TransformerDecoderLayer(d_model=self.dim, nhead=nhead, dim_feedforward=dim_feedforward),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(self.dim, num_classes)\n",
        "\n",
        "        # It is empirically important to initialize weights properly\n",
        "        self.init_weights()\n",
        "    \n",
        "    def init_weights(self):\n",
        "        pass\n",
        "    \n",
        "    def forward():\n",
        "        pass\n",
        "\n",
        "    def encode():\n",
        "        pass\n",
        "\n",
        "    def decode():\n",
        "        pass\n",
        "\n",
        "    def predict():\n",
        "        pass\n",
        "\n",
        "\n",
        "model = Transformer(num_classes=C, max_output_length=y.shape[1])\n",
        "logits = model(x, y[:, :-1])\n",
        "print(x.shape, y.shape, logits.shape)\n",
        "print(x[0:1])\n",
        "print(model.predict(x[0:1]))"
      ],
      "metadata": {
        "id": "zKF5snKMulqT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}