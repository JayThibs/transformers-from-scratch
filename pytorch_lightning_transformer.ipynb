{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_lightning_transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMVRaTbQPga8DvFJzmUHSRI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/transformers-from-scratch/blob/main/pytorch_lightning_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers with PyTorch Lightning\n",
        "\n",
        "References:\n",
        "\n",
        "* [Simple PyTorch Transformer Example with Greedy Decoding](https://colab.research.google.com/drive/1swXWW5sOLW8zSZBaQBYcGQkQ_Bje_bmI) by Sergey Karayev from Full Stack Deep Learning\n",
        "* [The Annotated Transformer ++](https://github.com/gordicaleksa/pytorch-original-transformer/blob/main/The%20Annotated%20Transformer%20%2B%2B.ipynb) by gordicaleksa / The AI Epiphany\n",
        "* [Various Annotated Transformer PyTorch Papers](https://nn.labml.ai/transformers/index.html) by labml.ai\n",
        "\n",
        "This notebook provides a simple, self-contained example of Transformer models:\n",
        "\n",
        "* using both the encoder and decoder parts\n",
        "* greedy decoding at inference time\n",
        "\n",
        "For the first part of the notebook, we'll train on a simple synthetic example, and use PyTorch Lightning since it will greatly simplify the training loop.\n",
        "\n",
        "When the first transformer paper came out (Attention Is All You Need), the authors used the transformer architecture for machine translation. This means that they needed both the encoder and decoder parts of the architecture to first encode the text, and then decode (generate) the translation.\n",
        "\n",
        "After that paper, researchers realized that they could use the encoder and decoder separately in order to create models for approaching different tasks. This led to the emergence of BERT-like models (encoder / non-autoregressive) and GPT-like models (decoder / autoregressive).\n",
        "\n",
        "However, we'll be going over each part of the entire transformer.\n",
        "\n",
        "Note: Autoregressive means that model only takes into account the text or context that came before our current prediction. Each new prediction is taken into account in the next prediction. Non-autoregressive models take the entire surrounding context! So, a model like BERT uses bi-directionality (that's what the B stands for) and takes in the entire surrounding context for word prediction when trying to predict a masked word. This makes it so that GPT is great at generating text, while BERT is great at taking in an entire piece of text and classifying it."
      ],
      "metadata": {
        "id": "S1mc18GM-tZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3Cx0eeAlDfFw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}